# GPT-4 Productivity Study

TODO: front matter, author, affiliation, abstract, keywords, topic codes...

## Background and Introduction

notes:

1.  there seem to be four interesting aspects to this study:

    1. We focus on GPT-4 with a novel Plugin Forest prompting strategy.

       1. There's lots of conversation about AI and productivity, but what about GPT-4 in particular? Often we see discussions of ChatGPT, but under the hood they are using GPT-3.5 which is importantly different and inferior. GPT-4 is not only preferred to other large language models for performance reasons, it is in fact not even a large language model! It's a multimodal model, which represents an architectural step beyond "the era of large parameter count LLMs" as Sam Altman has discussed.
       2. We know that prompt techniques cause significant performance variation for a given model, and tree of thoughts is a leading-edge prompt strategy. ChatGPT Plugins are a GPT-4-specific augmentation that improves GPT-4 performance further for certain tasks, including literature review. Of note, plugins give GPT-4 access to some research that it would otherwise not be able to access. The Plugin Forest utilizes plugins together with tree of thought prompting, so it represents an extremely high bar of expected output quality with a high degree of standardization and reproducibility. As one example of quality difference, contrasting with other prompting approaches, this strategy notably provided zero citation hallucinations.
       <!-- TODO: verify that none of the citations were hallucinated. I think so, but if we are going to claim that directly, let's triple-check -->

    2. We are the first to calibrate GPT-4 on the particular task of economic research writing for publication. We know that models perform differently for different tasks due to training sample variation, differential human feedback in RLHF, and for other reasons. Unlike other papers which provide business-oriented research productivity estimates, our paper is of specific interest to other researchers in the academy. GPT-4 has previously shown broad competency across many skills, but competency tends to devolve with the required depth of expertise. By selecting a particular field of research at the publication level, we probe multimodel capabilities to an extent previously unexplored and with specific practical value to many researchers.

    3. i forgot 3 and 4...but the two above are great already

## Methodology

TODO

## Results

TODO

## Conclusion

TODO
