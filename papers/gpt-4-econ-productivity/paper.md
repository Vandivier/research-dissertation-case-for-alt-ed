# GPT-4 Calibration as a Research Assistant in Economics

## Abstract

This paper defines the Plugin Forest, a best-practice prompt engineering strategy for research.
We then leverage crowd feedback to comparatively calibrate GPT-4 summary literature reviews against human-authored documents related to a variety of research topics within the field of economics.
We find that reviews constructed with a Plugin Forest obtain an average quality rating equal to doctorate-level researchers in the field.
Further, the high-quality documents generated by GPT-4 exhibited smaller quality variation compared to human-authored documents.
Notably, GPT-4 using the Plugin Forest technique generated literature reviews with zero hallucinated citations.
We find that GPT-4 quality is sensitive to paper topic, with lower performance for lesser-studied topics.
Further, graders were reliably unable to determine when a document was authored by GPT-4, even when the grader had a graduate education in the field.
Graders did not identify GPT-4 authored material as written by a doctorate, however, indicating a notable difference in style even while quality remains matched.
We conclude that authoring research using GPT-4 and the Plugin Forest reliably enhances researcher productivity with no loss in quality for a variety of tasks.

Authors: John, Josh

Keywords:

- GPT-4
- ChatGPT
- Economic Literature Review
- AI in Academic Research
- AI Augmentation
- Multimodal Models
- Research Productivity
- Prompt Engineering
- Tree of Thoughts
- In-Context Learning
- Mixture of experts

JEL Codes:

- C88 - Other Computer Software
- A11 - Role of Economics; Role of Economists; Market for Economists
- C80 - General (Data Collection and Data Estimation Methodology; Computer Programs)
- B41 - Economic Methodology
- I23 - Higher Education and Research Institutions
- A10, B41, J23, O3 [used by Korinek]

## Background and Introduction

Economists are told that generative artificial intelligence tools including ChatGPT are on the one hand [increasingly prolific and useful tool](https://www.aeaweb.org/articles?id=10.1257/jel.20231736), while on the other hand there are critical voices calling such tools risky on quality or even legally problematic[[Bilal](https://twitter.com/MushtaqBilalPhD/status/1639902509274726400), [Buchanan et al](https://journals.sagepub.com/doi/10.1177/05694345231218454)]. There is a lack of empirical work on productivity and output quality, a lack of standard usage guidance, and a mixing up of large language models, ChatGPT, and other tools of artificial intelligence that have distinct usage patterns and productivity implications.

This paper is the first to provide quantitative data calibrating GPT-4 against doctorate-produced research. We provide a level of utility and recognize a level of nuance by task that is absent in many discussions of research productivity by looking at a variety of topics within the field of economics. This paper describes a new best practice and achieves results that represent the cutting edge of generative artificial intelligence through the use of GPT-4 with a novel prompt engineering technique called a Plugin Forest.

We find that GPT-4 with a Plugin Forest produces literature review summaries of comparable point-estimated quality to a doctorate-level researcher while retaining lower variation in quality compared to a human author. Of note, zero hallucinated citations were generated following this process, in contrast to results frequently attributed to ChatGPT, but in fact generally attributable to GPT-3.5, a substantially inferior and architecturally distinct model compared to GPT-4.

We describe nuances with these results, such as weaker performance for lesser-published topics, we describe novel use cases for GPT-4 that add to known large language model use cases for research, and we make use of GPT-4 plugins with systematic and reprodicible plugin selection, a topic for which there is currently no published empirical work. We conclude with a discussion on expected future productivity trends and open research areas, providing an evidence-based case that productivity over time in the space of generative artificial intelligence is expected to grow at a modest pace.

- TODO: novel tasks include reading and generating images like diagrams, executing, interpreting, and generating code, and translating documents between technical formats like latex, html, and markdown, and knowledge retrieval
  - GPT-4 provides unique productivity opportunities for researchers due to its ability to generate, improve, and execute code for data analysis and both read and create figures and tables. These tasks are not possible for pure large language models.
- TODO: more discussion on what a plugin forest is and why it's awesome
  - more than half a dozen best practices built in, such as tree of thoughts, roles, mixture of experts, and chain of thought prompting, plus making systematic and reprodcible use of gpt-4 plugins
- we know prompt strategy drives productivity, but research on GPT-4 that does exist fails to incorporate best-practice prompt techniques and leverage GPT-4 capabilities like plugins. Plugins importantly provide access to academic papers, and we know from research like "textbooks are all you need" that access to academic material is a very important driver of producing high-quality academic-level results.
- GPT-4 performance has notably varied over time. [Chen et al](https://arxiv.org/abs/2307.09009) notes significant performance decreases for some tasks, and this observation has been independently replicated by the [ChatBot Arena Leaderboard Project](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard). Importantly, this model shift took place during the observation period for the present study, and as a result we report a coefficient for performance shift relevant to our particular task in economic research.
  - Our observation period was May-July and the ChatGPT UI at that time used the March GPT-4 API, then switched to the June GPT-4 api during the observation period.

## Methodology

### Overview

This study adopts a comparative approach to evaluate the efficacy of GPT-4 as a tool for conducting economic literature reviews, juxtaposed against human researchers with varying academic credentials in economics. The methodology encompasses participant engagement, GPT-4 interaction, review process, and statistical analysis.

### Participant Recruitment and Task Distribution

#### Selection Criteria

- Participants were selected based on their academic background in economics, comprising one bachelor's degree holder, one master's degree holder, and two doctoral degree holders.

#### Task Description

- Each participant was assigned the task of drafting a single-page summary literature review. The scope and format of these reviews were intended to parallel the background section of a standard scholarly article.

### Integration of GPT-4

#### ChatGPT Interface Use

- The principal investigator engaged with GPT-4 using the ChatGPT web interface to generate additional literature reviews corresponding to the research questions assigned to human participants.

#### Application of Plugin Forest Technique

- A novel 'Plugin Forest' approach was employed, which entailed the configuration of multiple plugin collections and the execution of a 'tree of thoughts' prompting strategy. The resultant data from each collection was subsequently synthesized.

### Blind Review and Scoring Mechanism

#### Blind Assessment

- To ensure impartiality, participants were unaware of the authors of the papers they reviewed, except for their own contributions. The principal investigator, however, was privy to the authorship details of all submissions.

#### Scoring Parameters

- Participants rated the papers on a scale of 1 to 10, focusing on two aspects: perceived quality of the content and likelihood of the paper being generated by GPT-4. This dual-scale assessment aimed to minimize any bias in quality perception influenced by the assumed origin of the paper.

### Editing Process and Selection for Analysis

#### Randomized Paper Selection

- A random selection algorithm was used to pick one GPT-4 authored review per research question for further analysis. These selected pieces underwent editing by the principal investigator.

#### Editing Workflow Evaluation

- The study considered both the unedited (naive) AI-authored submissions and the potential benefits of AI-assisted editing in refining the final output.

### Analytical Approach

#### Statistical Comparison

- The analysis involved a statistical comparison of the ratings assigned by participants, accounting for potential biases, including those inherent to individual participants and the principal investigator.

#### Methodological Integrity

- The study adhered to rigorous standards of academic research, ensuring the reliability and validity of the findings through methodical data collection and analysis.

### Ethical Compliance

#### Informed Consent

- All participants were provided with a comprehensive informed consent form, elucidating the academic and commercial use of their input in an anonymized format.

#### Voluntariness of Participation

- Participation was voluntary, with the provision for participants to withdraw at any stage of the study. However, data collected prior to withdrawal were retained for analysis as per the study's protocol.

The methodology of this research was meticulously designed to ensure a thorough and unbiased evaluation of GPT-4's capabilities in comparison with human expertise in economic research writing. The study's structured approach aimed at delivering insightful conclusions on the role and effectiveness of AI in academic literature review processes.

## Results

#### Case Study Results

#### Generalized Case Study

- People couldn't tell it was GPT-4
- Even field doctorates couldn't tell

#### Regression Results

- 3 model table
- Regression of assessed education level on GPT Authorship

## Conclusion

TODO: the draft-prep-notes still has two conclusion sections

## Appendix A: Questionnaire

### Title:

GPT-4 Survey

### Initial Message to Participants:

The purpose of this survey is to determine the academic writing ability of GPT-4 compared to humans. This survey will present 18 articles, each about a page in length, written by a mix of humans at a variety of educational levels and GPT-4. Please read each article and rate the quality. This survey is expected to take 30-90 minutes, varying mainly by reading speed.

### Question 1

On a scale of 1-10, with one being the least attention and 10 being the most attention, please indicate how much attention you applied while completing this study

### Question 2

Enter your email or Participant ID to receive a participation reward.

### Question 3

What is your highest level of education?

Responses:

1. High School or Less
2. Some College
3. An Undergraduate Degree
4. A Graduate Degree
5. A Ph.D.

### Question 4

Do you have a postsecondary degree in Economics? (Y/N)

### Question 5-23

This question is repeated for a DOCUMENT_ID ranging from 1 to 18

For Document ID [DOCUMENT_ID], please answer the following three questions using a comma-separated format.

1. What education level does the writer appear to have? Use ""u"" for undergraduate or lower, ""m"" for the master's level, or ""p"" for Ph.D. or higher.
2. Rate the article quality on a scale from 1-10.
3. Rate the likelihood that the article is written by GPT-4 on a scale from 1-10.

An example answer would be ""u,1,1""

## Appendix B: Documents and ChatGPT Source Threads

1. [Review of Macroeconomic Indicators](https://osf.io/f5tbz?view_only=ca664b55eae243339f13c59cc5dea3ec)
   1. [Plugin Forest Identification](https://chat.openai.com/share/85bbd3a0-b7ec-4751-be37-63ccb050a88c)
   2. [Journalist Persona](https://chat.openai.com/share/799526c8-5099-4b5a-96c2-0a9d252fab92)
   3. [Professor Persona](https://chat.openai.com/share/1c5ce089-15e6-4378-9e44-a2c4f9d4cbf5)
   4. [Data Scientist Persona](https://chat.openai.com/share/f1de01cb-2885-4619-bf2d-5a8286b8d80b)
   5. [Ph.D. Student in Economics Persona](https://chat.openai.com/share/24b89476-a314-4656-b7c8-f3d734bc3df0)
   6. [Synthesis Draft](https://chat.openai.com/share/f7c6aa68-0b23-4098-beec-49a3ee8a6633)
2. [Review of Gender Effects in the Post-Pandemic Labor Market](https://osf.io/9xeju?view_only=ca664b55eae243339f13c59cc5dea3ec)
   1. [Plugin Forest Identification](https://chat.openai.com/share/bfec6a29-b0bd-4288-ae08-a84143104ebb)
   2. [Policy Analyst Persona](https://chat.openai.com/share/4714fbd6-d87f-4708-97a3-41b1614dc96c)
   3. [Professor Persona](https://chat.openai.com/share/9dd05687-7eb9-4d45-9c40-5f457c9b20fe)
   4. [Author Persona](https://chat.openai.com/share/60e546aa-7749-4567-8fa0-bc1c0c9d0cbb)
   5. [Synthesis Draft](https://chat.openai.com/share/90a2bf74-bf30-4f41-8137-719d47a019ec)
3. [Review of LLM Best Practices](https://osf.io/37a9g?view_only=ca664b55eae243339f13c59cc5dea3ec)\*
   1. [Plugin Forest Identification](https://chat.openai.com/share/82c30d4b-43ac-4979-9baf-36765b3add59)
   2. [Journalist Persona](https://chat.openai.com/share/aa50dc44-413b-42b7-b213-8fc54f64a02a)
   3. [Professor Persona](https://chat.openai.com/share/4542f4a7-584a-4064-8550-2e00a89f4abb)
   4. [Data Scientist Persona](https://chat.openai.com/share/4cdb8a6c-cefa-420e-a2ed-8313e8b819ae)
   5. [Policy Analyst Persona](https://chat.openai.com/share/6ba03cb0-26c0-4009-bffb-b77bad736999)
   6. [Synthesis Draft](https://chat.openai.com/share/60bffb60-d0e0-4537-8b23-6e4de29418ce)\*
4. [Comparative Review of the Austrian and Neoclassical Schools](https://osf.io/mkn5f?view_only=ca664b55eae243339f13c59cc5dea3ec)\*
   1. [Plugin Forest Identification](https://chat.openai.com/share/f734a988-c662-4a52-aadd-7492bac1fbbe)\*
   2. [Policy Analyst Persona](https://chat.openai.com/share/df3f074f-1dec-4763-9848-c28d2cdc6faf)\*
   3. [Professor Persona](https://chat.openai.com/share/9ccd7128-2d89-4f9a-bde7-21333d964144)\*
   4. [Education Researcher Persona, Part 1: As Assigned](https://chat.openai.com/share/100a10d6-19c7-4999-b135-abe1dc1a7eee)\*
   5. [Education Researcher Persona, Part 2: With WebPilot](https://chat.openai.com/share/100a10d6-19c7-4999-b135-abe1dc1a7eee)\*
   6. [Education Researcher Persona, Part 3: Browser Exploration](https://chat.openai.com/share/c685cdf0-61ea-43e4-8091-ec0c94a9a1ee)\*
   7. [Education Researcher Persona, Part 4: With BrowerOp Plugin](https://chat.openai.com/share/097687df-8c2a-49a1-afa3-4467d3d4d29d)\*
   8. [Synthesis Draft](https://chat.openai.com/share/9d18ab22-4001-400b-875d-2f8c5bef53fc)\*

\*Generated using GPT-4 with the July 20th, 2023 version of ChatGPT. All others use the version from May 24th, 2023.
