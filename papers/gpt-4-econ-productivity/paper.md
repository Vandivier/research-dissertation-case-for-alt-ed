# GPT-4 Calibration as a Research Assistant in Economics

## Abstract

This paper defines the Plugin Forest, a best-practice prompt engineering strategy for research.
We then leverage crowd feedback to comparatively calibrate GPT-4 summary literature reviews against human-authored documents related to a variety of research topics within the field of economics.
We find that reviews constructed with a Plugin Forest obtain an average quality rating equal to doctorate-level researchers in the field.
Further, the high-quality documents generated by GPT-4 exhibited smaller quality variation compared to human-authored documents.
Notably, GPT-4 using the Plugin Forest technique generated literature reviews with zero hallucinated citations.
We find that GPT-4 quality is sensitive to paper topic, with lower performance for lesser-studied topics.
Further, graders were reliably unable to determine when a document was authored by GPT-4, even when the grader had a graduate education in the field.
Graders did not identify GPT-4 authored material as written by a doctorate, however, indicating a notable difference in style even while quality remains matched.
We conclude that authoring research using GPT-4 and the Plugin Forest reliably enhances researcher productivity with no loss in quality for a variety of tasks.

Authors: John, Josh

Keywords:

- GPT-4
- ChatGPT
- Economic Literature Review
- AI in Academic Research
- AI Augmentation
- Multimodal Models
- Research Productivity
- Prompt Engineering
- Tree of Thoughts
- In-Context Learning
- Mixture of experts

JEL Codes:

- C88 - Other Computer Software
- A11 - Role of Economics; Role of Economists; Market for Economists
- C80 - General (Data Collection and Data Estimation Methodology; Computer Programs)
- B41 - Economic Methodology
- I23 - Higher Education and Research Institutions

## Background and Introduction

- lots of research on large language models, but less on multimodal models like GPT-4
- we know generative ai task performance varies widely based on the particular task. today there is research on productivity does exist for knowledge work and research in general, and there is also some research on generative ai for financial analysis, but there is no direct research on the quality of economic research produced by GPT-4. We provide this and dive further to the level of research topics, providing nuance about when and how to use GPT-4 rather than mere top-line productivity estimates.
- this is really important because GPT-4 has higher task performance and broader task usage compared to LLMs
- GPT-4 provides unique productivity opportunities for researchers due to its ability to generate, improve, and execute code for data analysis and both read and create figures and tables. These tasks are not possible for pure large language models.
- lots of discussion about ChatGPT which disregards GPT-4, the most powerful form of ChatGPT
- people say that chatgpt hallucinates and in many cases it has been directly contraindicated for academic work (eg, Buchanan and Shapoval https://economistwritingeveryday.com/2023/06/04/new-paper-with-evidence-that-chatgpt-hallucinates-nonexistent-citations/)
- we know prompt strategy drives productivity, but research on GPT-4 that does exist fails to incorporate best-practice prompt techniques and leverage GPT-4 capabilities like plugins. Plugins importantly provide access to academic papers, and we know from research like "textbooks are all you need" that access to academic material is a very important driver of producing high-quality academic-level results.
- GPT-4 performance has notably varied over time. [Chen et al](https://arxiv.org/abs/2307.09009) notes significant performance decreases for some tasks, and this observation has been independently replicated by the [ChatBot Arena Leaderboard Project](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard). Importantly, this model shift took place during the observation period for the present study, and as a result we report a coefficient for performance shift relevant to our particular task in economic research.
  - Our observation period was May-July and the ChatGPT UI at that time used the March GPT-4 API, then switched to the June GPT-4 api during the observation period.

## Methodology

### Overview

This study adopts a comparative approach to evaluate the efficacy of GPT-4 as a tool for conducting economic literature reviews, juxtaposed against human researchers with varying academic credentials in economics. The methodology encompasses participant engagement, GPT-4 interaction, review process, and statistical analysis.

### Participant Recruitment and Task Distribution

#### Selection Criteria

- Participants were selected based on their academic background in economics, comprising one bachelor's degree holder, one master's degree holder, and two doctoral degree holders.

#### Task Description

- Each participant was assigned the task of drafting a single-page summary literature review. The scope and format of these reviews were intended to parallel the background section of a standard scholarly article.

### Integration of GPT-4

#### ChatGPT Interface Use

- The principal investigator engaged with GPT-4 using the ChatGPT web interface to generate additional literature reviews corresponding to the research questions assigned to human participants.

#### Application of Plugin Forest Technique

- A novel 'Plugin Forest' approach was employed, which entailed the configuration of multiple plugin collections and the execution of a 'tree of thoughts' prompting strategy. The resultant data from each collection was subsequently synthesized.

### Blind Review and Scoring Mechanism

#### Blind Assessment

- To ensure impartiality, participants were unaware of the authors of the papers they reviewed, except for their own contributions. The principal investigator, however, was privy to the authorship details of all submissions.

#### Scoring Parameters

- Participants rated the papers on a scale of 1 to 10, focusing on two aspects: perceived quality of the content and likelihood of the paper being generated by GPT-4. This dual-scale assessment aimed to minimize any bias in quality perception influenced by the assumed origin of the paper.

### Editing Process and Selection for Analysis

#### Randomized Paper Selection

- A random selection algorithm was used to pick one GPT-4 authored review per research question for further analysis. These selected pieces underwent editing by the principal investigator.

#### Editing Workflow Evaluation

- The study considered both the unedited (naive) AI-authored submissions and the potential benefits of AI-assisted editing in refining the final output.

### Analytical Approach

#### Statistical Comparison

- The analysis involved a statistical comparison of the ratings assigned by participants, accounting for potential biases, including those inherent to individual participants and the principal investigator.

#### Methodological Integrity

- The study adhered to rigorous standards of academic research, ensuring the reliability and validity of the findings through methodical data collection and analysis.

### Ethical Compliance

#### Informed Consent

- All participants were provided with a comprehensive informed consent form, elucidating the academic and commercial use of their input in an anonymized format.

#### Voluntariness of Participation

- Participation was voluntary, with the provision for participants to withdraw at any stage of the study. However, data collected prior to withdrawal were retained for analysis as per the study's protocol.

The methodology of this research was meticulously designed to ensure a thorough and unbiased evaluation of GPT-4's capabilities in comparison with human expertise in economic research writing. The study's structured approach aimed at delivering insightful conclusions on the role and effectiveness of AI in academic literature review processes.

## Results

#### Case Study Results

#### Generalized Case Study

- People couldn't tell it was GPT-4
- Even field doctorates couldn't tell

#### Regression Results

- 3 model table
- Regression of assessed education level on GPT Authorship

## Conclusion

TODO: the draft-prep-notes still has two conclusion sections

## Appendices

- Questionnaire
- Links to ChatGPT Source Threads
- Links to OSF Summary Literature Review Documents
